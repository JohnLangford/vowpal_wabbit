
chiczhan@chiczhan-ubuntu:~/datasets$ ~/local_vw/vowpal_wabbit/vowpalwabbit/vw --cbify 10 -d text_lownoise_m.vw --warm_start 15 --choices_lambda 10 --cb_type ips
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = text_lownoise_m.vw
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        4        1      101
1.000000 1.000000            2            2.0       10        4      101
0.750000 0.500000            4            4.0        7       10      101
0.625000 0.500000            8            8.0        8        8      101
0.130435 0.026316           16           46.0       10       10      101
0.629630 1.000000           18          108.0        2       10      101
0.560345 0.500000           22          232.0        3        7      101
0.529167 0.500000           30          480.0        9        8      101
0.355533 0.187500           46          976.0        8        8      101
0.365346 0.375000           78         1968.0        2        7      101
0.480010 0.593750          142         3952.0        9        5      101
0.517424 0.554688          270         7920.0        8        8      101
0.496973 0.476562          526        15856.0        8        8      101
0.472107 0.447266         1038        31728.0        2        9      101
0.441124 0.410156         2062        63472.0        1        1      101
0.348968 0.256836         4110       126960.0        8        1      101
0.242348 0.135742         8206       253936.0        8        8      101

finished run
number of examples per pass = 10000
passes used = 1
weighted example sum = 309550.000000
weighted label sum = 0.000000
average loss = 0.209223
total feature number = 1010000
chiczhan@chiczhan-ubuntu:~/datasets$ ~/local_vw/vowpal_wabbit/vowpalwabbit/vw --cbify 10 -d text_lownoise_m.vw --warm_start 15 --choices_lambda 10 --cb_type ips --no_supervised
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = text_lownoise_m.vw
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        4        1      101
1.000000 1.000000            2            2.0       10        1      101
1.000000 1.000000            4            4.0        7        1      101
0.875000 0.750000            8            8.0        8        1      101
0.978261 1.000000           16           46.0       10        1      101
0.990741 1.000000           18          108.0        2        3      101
0.995690 1.000000           22          232.0        3        7      101
0.933333 0.875000           30          480.0        9        7      101
0.871926 0.812500           46          976.0        8        8      101
0.715955 0.562500           78         1968.0        2        2      101
0.693826 0.671875          142         3952.0        9        3      101
0.647601 0.601562          270         7920.0        8        8      101
0.648020 0.648438          526        15856.0        8        8      101
0.666793 0.685547         1038        31728.0        2        6      101
0.622936 0.579102         2062        63472.0        1        1      101
0.513603 0.404297         4110       126960.0        8        1      101
0.413289 0.312988         8206       253936.0        8        8      101

finished run
number of examples per pass = 10000
passes used = 1
weighted example sum = 309550.000000
weighted label sum = 0.000000
average loss = 0.354960
total feature number = 1010000
chiczhan@chiczhan-ubuntu:~/datasets$ ~/local_vw/vowpal_wabbit/vowpalwabbit/vw --cbify 10 -d text_lownoise_m.vw --warm_start 15 --choices_lambda 10 --cb_type ips --no_bandit
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = text_lownoise_m.vw
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0        4        1      101
1.000000 1.000000            2            2.0       10        4      101
0.750000 0.500000            4            4.0        7       10      101
0.625000 0.500000            8            8.0        8        8      101
0.375000 0.125000           16           16.0       10       10      101
0.437500 0.500000           32           32.0        8        8      101
0.406250 0.375000           64           64.0        3        5      101
0.476562 0.546875          128          128.0        3        5      101
0.480469 0.484375          256          256.0       10       10      101
0.443359 0.406250          512          512.0        2       10      101
0.445312 0.447266         1024         1024.0        1        1      101
0.438965 0.432617         2048         2048.0        9        5      101
0.430176 0.421387         4096         4096.0        4        4      101
0.423340 0.416504         8192         8192.0       10       10      101

finished run
number of examples per pass = 10000
passes used = 1
weighted example sum = 10000.000000
weighted label sum = 0.000000
average loss = 0.426300
total feature number = 1010000

chiczhan@chiczhan-ubuntu:~/datasets$ ~/local_vw/vowpal_wabbit/vowpalwabbit/vw --cbify 10 -d text_highnoise_m.vw --warm_start 40 --choices_lambda 20 --cb_type ips 
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = text_highnoise_m.vw
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.000000 0.000000            1            1.0        1        1       34
0.500000 1.000000            2            2.0        7        1       41
0.750000 1.000000            4            4.0        7        9       36
0.750000 0.750000            8            8.0        2        5       38
0.750000 0.750000           16           16.0        9        9       40
0.812500 0.875000           32           32.0        8        3       45
0.991533 0.997090           41         1063.0        1        2       39
0.668060 0.500000           43         3109.0        5        5       33
0.714623 0.750000           47         7201.0        7        7       35
0.600455 0.500000           55        15385.0       10       10       42
0.516455 0.437500           71        31753.0        9        9       32
0.587418 0.656250          103        64489.0        7        3       42
0.629966 0.671875          167       129961.0        6        6       41
0.678446 0.726562          295       260905.0        2        6       37
0.684938 0.691406          551       522793.0        6        8       42
0.706747 0.728516         1063      1046569.0        8        9       43
0.677090 0.647461         2087      2094121.0        2        2       37
0.672040 0.666992         4135      4189225.0        1        1       45
0.663167 0.654297         8231      8379433.0       10        5       33

finished run
number of examples per pass = 10000
passes used = 1
weighted example sum = 10189120.000000
weighted label sum = 0.000000
average loss = 0.663153
total feature number = 390046
chiczhan@chiczhan-ubuntu:~/datasets$ ~/local_vw/vowpal_wabbit/vowpalwabbit/vw --cbify 10 -d text_highnoise_m.vw --warm_start 40 --choices_lambda 20 --cb_type ips --no_supervised
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = text_highnoise_m.vw
num sources = 1
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
0.000000 0.000000            1            1.0        1        1       34
0.500000 1.000000            2            2.0        7        1       41
0.750000 1.000000            4            4.0        7        1       36
0.875000 1.000000            8            8.0        2        1       38
0.937500 1.000000           16           16.0        9        1       40
0.937500 0.937500           32           32.0        8        1       45
0.035748 0.007759           41         1063.0        1        1       39
0.670312 1.000000           43         3109.0        5        2       33
0.715595 0.750000           47         7201.0        7        4       35
0.866883 1.000000           55        15385.0       10        4       42
0.903285 0.937500           71        31753.0        9        5       32
0.888927 0.875000          103        64489.0        7        2       42
0.874039 0.859375          167       129961.0        6        6       41
0.913731 0.953125          295       260905.0        2        2       37
0.876718 0.839844          551       522793.0        6        7       42
0.864128 0.851562         1063      1046569.0        8        6       43
0.851980 0.839844         2087      2094121.0        2        4       37
0.848841 0.845703         4135      4189225.0        1        1       45
0.837139 0.825439         8231      8379433.0       10        5       33

finished run
number of examples per pass = 10000
passes used = 1
weighted example sum = 10189120.000000
weighted label sum = 0.000000
average loss = 0.834037
total feature number = 390046

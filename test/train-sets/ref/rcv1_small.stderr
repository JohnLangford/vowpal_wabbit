using l2 regularization = 1
enabling BFGS based optimization **without** curvature calculation
Num weight bits = 20
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
m = 7
Allocated 72M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size 
creating cache_file = train-sets/rcv1_small.dat.cache
Reading datafile = train-sets/rcv1_small.dat
num sources = 1
 1 0.69315   	0.00266   	0.87764   	          	          	          	2.24708   	776.93237 	0.39057   
 3 0.51357   	0.00493   	0.24713   	 0.523903  	0.088793  	          	          	40.10523  	1.00000   
 4 0.48798   	0.00242   	0.08860   	 0.272538  	-0.413836 	          	          	2.57582   	1.00000   
 5 0.47891   	0.00007   	0.00670   	 0.596746  	0.184553  	          	          	0.47271   	1.00000   
 6 0.47758   	0.00000   	0.00243   	 0.704293  	0.405333  	          	          	0.71132   	1.00000   
 7 0.47683   	0.00000   	0.00047   	 0.594194  	0.187044  	          	          	0.11225   	1.00000   
 8 0.47672   	0.00000   	0.00003   	 0.569958  	0.139901  	          	          	0.00602   	1.00000   

finished run
number of examples = 8000
weighted example sum = 8000
weighted label sum = -656
average loss = 0.458978
best constant = -0.164369
best constant's loss = 0.689781
total feature number = 629912

using l1 regularization = 0.5
using l2 regularization = 0.5
final_regressor = models/contcb_linear.model
Num weight bits = 18
learning rate = 0.0005
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = train-sets/contcb_linear.dat
num sources = 1
Enabled reductions: contcb
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
4.668577 4.668577            1            1.0 {-1,4.66858,1}      0,1        5
23.385854 42.103130            2            2.0 {1.47697,42.1031,1} 0.476973,1        5
13.537732 3.689611            4            4.0 {-5.42052,7.3784,1} -4.42052,1        5
7.429229 1.320725            8            8.0 {-3.44708,2.01418,1} -4.44708,1        5
5.457091 3.484953           16           16.0 {-5.6576,3.79409,1} -4.6576,1        5
4.108892 2.760692           32           32.0 {-6.44126,9.86129,1} -5.44126,1        5
3.451741 2.794590           64           64.0 {-4.95412,11.6047,1} -3.95412,1        5
2.971609 2.491478          128          128.0 {-2.02025,1.30061,1} -3.02024,1        5
2.749312 2.527015          256          256.0 {-3.51401,1.20748,1} -4.51401,1        5

finished run
number of examples = 500
weighted example sum = 500.000000
weighted label sum = 500.000000
average loss = 2.398176
total feature number = 2500


#include <float.h>

#include "vw.h"
#include "reductions.h"
#include "cb_algs.h"
#include <sstream>

using namespace LEARNER;

#define CB_TYPE_DR 0
#define CB_TYPE_DM 1
#define CB_TYPE_IPS 2

using namespace CB;

namespace CB_ADF{
	struct label
	{
		v_array<cb_class> costs;
		size_t cb_type;
	};
}

struct cb_adf {  // define a contextual bandits setting that has action-dependent features
	size_t cb_type;
	uint32_t num_actions;
	COST_SENSITIVE::label cb_adf_cs_ld;
	LEARNER::base_learner* scorer;

	// should add an array of cb_label.
	v_array<CB::label> array;

	// suggest to add an array of cs_label.
	v_array<COST_SENSITIVE::label> cs_label_array;

	cb_class* known_cost;
};

void gen_cs_example_ips(cb_adf& c, v_array<example*> examples, v_array<COST_SENSITIVE::label> array)
{
	for (example **ec = examples.begin; ec != examples.end; ec++)
	{
		// Get CB::label for each example/line.
		CB::label ld = (**ec).l.cb;

		if ( ld.costs.size() == 1 )  // 2nd line
		{
			COST_SENSITIVE::wclass wc;
			wc.x = ld.costs[0].cost / ld.costs[0].probability;

			COST_SENSITIVE::label cs_ld;
			cs_ld.costs.push_back(wc);

			array.push_back(cs_ld);
		}
		else if (ld.costs.size() == 0)
		{
			COST_SENSITIVE::wclass wc;
			wc.x = 0.;

			if (true)  // compare if (**ec).tag == "shared"
			{
				COST_SENSITIVE::label cs_ld;
				cs_ld.costs.push_back(wc);
				array.push_back(cs_ld);
			}
			else
			{
				// In this case, push in an instance of wclass with maximum cost as merely a placeholder.
				wc.x = FLT_MAX;
				COST_SENSITIVE::label cs_ld;
				cs_ld.costs.push_back(wc);
				array.push_back(cs_ld);
			}
		}
	}
    
}

template <bool is_learn>
void gen_cs_label(cb_adf& c, example& ec, v_array<COST_SENSITIVE::label> array)
{
	COST_SENSITIVE::wclass wc;
	wc.wap_value = 0.;

	//get cost prediction for this label
	// num_actions should be 1 effectively.
	// my get_cost_pred function will use 1 for 'index-1+base'
	wc.x = CB_ALGS::get_cost_pred<is_learn>(c.scorer, c.known_cost, ec, 1, 1);

	//add correction if we observed cost for this action and regressor is wrong
	if (c.known_cost != nullptr && c.known_cost->action == label)
	{
		c.nb_ex_regressors++;
		c.avg_loss_regressors += (1.0f / c.nb_ex_regressors)*
			((c.known_cost->cost - wc.x)*(c.known_cost->cost - wc.x)
			- c.avg_loss_regressors);
		c.last_pred_reg = wc.x;
		c.last_correct_cost = c.known_cost->cost;

		wc.x += (c.known_cost->cost - wc.x) / c.known_cost->probability;
	}

	// do two-step pushes as for learn.
	COST_SENSITIVE::label cs_ld;
	cs_ld.costs.push_back(wc);
	array.push_back(cs_ld);
}

void gen_cs_example_dr(cb_adf& c, v_array<example*> examples, v_array<COST_SENSITIVE::label> array)
{
	for (example **ec = examples.begin; ec != examples.end; ec++)
	{
		// Get CB::label for each example/line.
		CB::label ld = (**ec).l.cb;

		if (ld.costs.size() == 1)  // 2nd line
		{
			gen_cs_label<true>(c, **ec, array);
		}
		else if (ld.costs.size() == 0)
		{
			gen_cs_label<false>(c, **ec, array);
		}
	}
}

inline bool observed_cost(cb_class* cl)
{
	//cost observed for this action if it has non zero probability and cost != FLT_MAX
	return (cl != nullptr && cl->cost != FLT_MAX && cl->probability > .0);
}

cb_class* get_observed_cost(CB::label ld)
{
	for (cb_class *cl = ld.costs.begin; cl != ld.costs.end; cl++)
	{
		if (observed_cost(cl))
			return cl;
	}
	return nullptr;
}


void predict(cb_adf& mydata, base_learner& base, v_array<example*> examples)
{

	// m2: still save, store, and restore
	// starting with 3 for loops
	// first of all, clear the container mydata.array.
	mydata.array.erase();

	// 1st: save cb_label (into mydata) and store cs_label for each example, which will be passed into base.learn.
	size_t index = 0;
	for (example **ec = examples.begin; ec != examples.end; ec++)
	{
		mydata.array.push_back((**ec).l.cb);
		(**ec).l.cs = mydata.cs_label_array[index];  // To be checked with John.
		index++;
	}


	// 2nd: predict for each ex
	// // call base.predict for each vw exmaple in the sequence
	for (example **ec = examples.begin; ec != examples.end; ec++)
	{
		base.predict(**ec);
	}

	// 3rd: restore cb_label for each example
	// (**ec).l.cb = mydata.array.element.
	size_t i = 0;
	for (example **ec = examples.begin; ec != examples.end; ec++)
	{
		(**ec).l.cb = mydata.array[i];
		i++;
	}

}
void learn(cb_adf& mydata, base_learner& base, v_array<example*> examples)
{
	// find the line/entry with cost and prob.
	CB::label ld;
	for (example **ec = examples.begin; ec != examples.end; ec++)
	{
		if ( (**ec).l.cb.costs.size() == 1 &&
			(**ec).l.cb.costs[0].cost != FLT_MAX &&
			(**ec).l.cb.costs[0].probability > 0)
		{
			ld = (**ec).l.cb;
		}
	}

	mydata.known_cost = get_observed_cost(ld);

	if (mydata.known_cost == nullptr)
	{
		cerr << "known cost is null." << endl;
	}

	// commented out. 
	// v_array<COST_SENSITIVE::label> array;
	// Use data structure mydata.cs_label_array to hold the cost - sensitive labels generated by gen_cs_example_ips.

	switch (mydata.cb_type)
	{
		case CB_TYPE_IPS:			
			gen_cs_example_ips(mydata, examples, mydata.cs_label_array);
			break;

		default:
			std::cerr << "Unknown cb_type specified for contextual bandit learning: " << mydata.cb_type << ". Exiting." << endl;
			throw exception();
	}

	if (mydata.cb_type != CB_TYPE_DM)
	{
		// 3 for-loops

		// first of all, clear the container mydata.array.
		mydata.array.erase();

		// 1st: save cb_label (into mydata) and store cs_label for each example, which will be passed into base.learn.
		size_t index = 0;
		for (example **ec = examples.begin; ec != examples.end; ec++)
		{
			mydata.array.push_back((**ec).l.cb);
			(**ec).l.cs = mydata.cs_label_array[index];  // To be checked with John.
			index++;
		}
		 

		// 2nd: learn for each ex
		// // call base.learn for each vw exmaple in the sequence
		for (example **ec = examples.begin; ec != examples.end; ec++)
		{
			base.learn(**ec);
		}

		// 3rd: restore cb_label for each example
		// (**ec).l.cb = mydata.array.element.
		size_t i = 0;
		for (example **ec = examples.begin; ec != examples.end; ec++)
		{
			(**ec).l.cb = mydata.array[i];
			i++;
		}
	}
}

// to be discussed later.
void finish_example(vw& all, cb_adf& c, example& ec)
{
	output_example(all, c, ec, ec.l.cb);
	VW::finish_example(all, &ec);
}

void output_example(vw& all, cb_adf& c, example& ec, CB::label& ld)
{
	// to be done.

}

void finish(cb_adf& c)
{
	c.cs_label_array.delete_v();
}


base_learner* cb_adf_setup(vw& all)
{
	// Check if command option "cb_adf" exists.
	if (missing_option<size_t, true>(all, "cb_adf", "Use contextual bandit learning with <k> costs"))
		return nullptr;

	// the only option specific to me is the cb_type 
	new_options(all, "CB options")
		("cb_type", po::value<string>(), "contextual bandit method to use in {ips,dm,dr}");
	add_options(all);

	// instantiate/allocate CB structure.
	cb_adf& c = calloc_or_die<cb_adf>();

	// the number of base regressors required.
	size_t problem_multiplier = 2;//default for DR
	if (all.vm.count("cb_type"))
	{
		std::string type_string;
		//store model type into model file.
		type_string = all.vm["cb_type"].as<std::string>();
		*all.file_options << " --cb_type " << type_string;

		if (type_string.compare("dr") == 0)
		{
			c.cb_type = CB_TYPE_DR;
		}
		else if (type_string.compare("ips") == 0)
		{
			c.cb_type = CB_TYPE_IPS;
			problem_multiplier = 1;
		}
		else
		{
			std::cerr << "warning: cb_type must be in {'ips','dr'}; resetting to dr." << std::endl;
			c.cb_type = CB_TYPE_DR;
		}
	}
	else
	{
		//by default use doubly robust
		c.cb_type = CB_TYPE_DR;
		*all.file_options << " --cb_type dr";
	}

	// make sure we have cost-senstive label-dependent-feature regressor
	// csoaa_ldf
	// Qustions: do we need to implement csoaa_ldf.cc?
	if (count(all.args.begin(), all.args.end(), "--csoaa") == 0)
	{
		all.args.push_back("--csoaa");
	}

	// set up my base learner.
	base_learner* base = setup_base(all);
	all.p->lp = CB::cb_label;

	learner<cb_adf>* l;

	// use base_learner to set up my own learner.
	// Problem is: there is no predict or learn function that has the same signature as required by init_learner.
	l = &init_learner(&c, base, learn, predict, problem_multiplier);
	
	l->set_finish_example(finish_example);

	l->increment = base->increment;

	c.scorer = all.scorer;

	l->set_finish(finish);
	//return my learner as base learner for the next tier.
	return make_base(*l);
}
